{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Prediction\n",
    "\n",
    "#### Dataset :\n",
    "Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) \n",
    "\n",
    "#### Objective : \n",
    "Using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n",
    "\n",
    " 1. To start -- `numpy` and `pandas` \n",
    " 2. For visualization -- `pyplot` subpackage of `matplotlib` \n",
    " 3. To add styling to the plots --  `rcParams`\n",
    " 4. For colors -- `rainbow`library\n",
    " 5. For implementing Machine Learning models and processing of data --`sklearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.cm import rainbow\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To split the available dataset for testing and training -- `train_test_split` method. \n",
    "2. To scale the features -- `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the Machine Learning algorithms needed: \n",
    "1. K Neighbors Classifier\n",
    "2. Support Vector Classifier\n",
    "3. Decision Tree Classifier\n",
    "4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset\n",
    "\n",
    "The dataset is stored in the file `dataset.csv`.\n",
    "Method to read the dataset -- pandas `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has now been loaded into the variable `dataset`.\n",
    "Taking a glimpse of the data using the `desribe()` and `info()` methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights Gained\n",
    "1. Total of 303 rows \n",
    "2. No missing values\n",
    "3. `13 features` along with one target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data using Visualization\n",
    "\n",
    "using visualizations to better understand the data\n",
    "1. Correlation (Colorbar)\n",
    "2. Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 14\n",
    "plt.matshow(dataset.corr())\n",
    "plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.xticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights gained\n",
    "Few features have negative correlation with the target value while some have positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights gained\n",
    "Each feature has a different range of distribution. \n",
    "\n",
    "Thus, using scaling before our predictions should be of great use.Also, the categorical features do stand out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data \n",
    "It's always a good practice to work with a dataset where the target classes are of approximately equal size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8,6\n",
    "plt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Target Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of each Target Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "After exploring the dataset,some categorical variables need to be converted into dummy variables and scaled up before training the Machine Learning models.\n",
    "\n",
    "1. using the `get_dummies` method to create dummy columns for categorical variables.\n",
    "2. using the `StandardScaler` from `sklearn` to scale my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardScaler = StandardScaler()\n",
    "columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Splitting the  dataset into training and testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['target']\n",
    "X = dataset.drop(['target'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Neighbors Classifier\n",
    "\n",
    "The classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = []\n",
    "for k in range(1,21):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    knn_scores.append(knn_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the scores for different neighbor values in the array `knn_scores`. I'll now plot it and see for which value of K did I get the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\n",
    "for i in range(1,21):\n",
    "    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\n",
    "plt.xticks([i for i in range(1, 21)])\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('K Neighbors Classifier scores for different K values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, it is clear that the maximum score achieved was `0.87` for the 8 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier\n",
    "\n",
    "There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_scores = []\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for i in range(len(kernels)):\n",
    "    svc_classifier = SVC(kernel = kernels[i])\n",
    "    svc_classifier.fit(X_train, y_train)\n",
    "    svc_scores.append(svc_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now plot a bar plot of scores for each kernel and see which performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = rainbow(np.linspace(0, 1, len(kernels)))\n",
    "plt.bar(kernels, svc_scores, color = colors)\n",
    "for i in range(len(kernels)):\n",
    "    plt.text(i, svc_scores[i], svc_scores[i])\n",
    "plt.xlabel('Kernels')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Support Vector Classifier scores for different kernels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear` kernel performed the best, being slightly better than `rbf` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier\n",
    "\n",
    "Here, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of `max_features` and see which returns the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = []\n",
    "for i in range(1, len(X.columns) + 1):\n",
    "    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "    dt_scores.append(dt_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\n",
    "for i in range(1, len(X.columns) + 1):\n",
    "    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\n",
    "plt.xticks([i for i in range(1, len(X.columns) + 1)])\n",
    "plt.xlabel('Max features')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Decision Tree Classifier scores for different number of maximum features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved the best accuracy at three values of maximum features, `2`, `4` and `18`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score for Decision Tree Classifier is {}% with {} maximum features.\".format(dt_scores[17]*100, [2,4,18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n",
    "\n",
    "Now, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores = []\n",
    "estimators = [10, 100, 200, 500, 1000]\n",
    "for i in estimators:\n",
    "    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    rf_scores.append(rf_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = rainbow(np.linspace(0, 1, len(estimators)))\n",
    "plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\n",
    "for i in range(len(estimators)):\n",
    "    plt.text(i, rf_scores[i], rf_scores[i])\n",
    "plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Random Forest Classifier scores for different number of estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum score is achieved when the total estimators are 100 or 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score for Random Forest Classifier is {}% with {} estimators.\".format(rf_scores[1]*100, [100, 500]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, Machine Learning is used to predict whether a person is suffering from a heart disease.\n",
    "\n",
    "Data Source: Kaggle\n",
    "\n",
    "Data Visulaization : colorbar , histogram , plots\n",
    "\n",
    "Data Processing: generated dummy variables for categorical features and scaled other features \n",
    "\n",
    "Machine Learning algorithms \n",
    " 1.  `K Neighbors Classifier`\n",
    " 2.  `Support Vector Classifier`\n",
    " 3.  `Decision Tree Classifier`\n",
    " 4.  `Random Forest Classifier`.\n",
    "  \n",
    "### Result\n",
    "1. parameters were across each model to improve their scores.\n",
    "2. `K Neighbors Classifier` achieved the highest score of `87%` with `8 nearest neighbors`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
